:::tip ElasticSearch版本
docker镜像：elasticsearch:7.9.1
:::


## 脚本环境准备
```bash
pip3 install elasticsearch==5.5.3 -i https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple
``````


## ES导入CSV
```python title="Python" {25-26}
from elasticsearch import Elasticsearch
from elasticsearch.helpers import scan
import csv
from typing import List, Optional


def es_to_csv(es_host: str,
              es_port: int,
              index_name: str,
              output_csv: str,
              fields: Optional[List[str]] = None,
              username: Optional[str] = None,
              password: Optional[str] = None,
              batch_size: int = 1000):
    """
    将Elasticsearch指定索引的全部数据导出为CSV文件（适配新版elasticsearch库）
    """
    # 构建ES连接URL
    es_url = f"http://{es_host}:{es_port}"

    # 初始化连接参数
    es_kwargs = {}
    # 处理认证（新版本中单独传递auth参数）
    if username and password:
        # es_kwargs["basic_auth"] = (username, password)
        es_kwargs["http_auth"] = (username, password)

    # 连接Elasticsearch
    es = Elasticsearch(es_url, **es_kwargs)

    # 检查索引是否存在
    if not es.indices.exists(index=index_name):
        raise ValueError(f"索引 '{index_name}' 不存在")

    # 检查连接是否成功
    if not es.ping():
        raise ConnectionError("无法连接到Elasticsearch服务")

    # 构建查询（匹配所有文档）
    query = {"query": {"match_all": {}}}

    # 扫描所有文档并写入CSV
    with open(output_csv, mode='w', newline='', encoding='utf-8') as csv_file:
        writer = None
        total_count = 0

        for hit in scan(es, index=index_name, query=query, size=batch_size):
            source_data = hit['_source']

            # 初始化CSV写入器
            if writer is None:
                headers = fields if fields else source_data.keys()
                writer = csv.DictWriter(csv_file, fieldnames=headers)
                writer.writeheader()

            # 准备数据行
            row_data = {field: source_data[field] for field in fields} if fields else source_data
            writer.writerow(row_data)
            total_count += 1

            # 打印进度
            if total_count % batch_size == 0:
                print(f"已导出 {total_count} 条数据...")

        print(f"导出完成！共导出 {total_count} 条数据到 {output_csv}")


if __name__ == "__main__":
    # 配置参数
    ES_HOST = "192.168.199.38"  # Elasticsearch主机
    ES_PORT = 9200  # Elasticsearch端口
    INDEX_NAME = "all_test"  # 替换为要导出的索引名
    OUTPUT_CSV = "/es_data.csv"  # 输出CSV文件路径
    USERNAME = None  # 如有认证，填写用户名
    PASSWORD = None  # 如有认证，填写密码
    # 如需指定导出字段，取消下面一行注释并修改字段列表
    # FIELDS = ["field1", "field2", "field3"]
    FIELDS = None  # 导出所有字段

    # 执行导出
    try:
        es_to_csv(
            es_host=ES_HOST,
            es_port=ES_PORT,
            index_name=INDEX_NAME,
            output_csv=OUTPUT_CSV,
            fields=FIELDS,
            username=USERNAME,
            password=PASSWORD
        )
    except Exception as e:
        print(f"导出失败: {str(e)}")

```

## CSV导入ES

:::tip
高亮部分可根据实际情况进行调整
:::

```python title="Python" {52-62,64-65}
import pandas as pd
from elasticsearch import Elasticsearch
from elasticsearch.helpers import bulk
from typing import Optional, Dict


def csv_to_es(csv_file: str,
              es_host: str,
              es_port: int,
              target_index: str,
              username: Optional[str] = None,
              password: Optional[str] = None,
              mapping: Optional[Dict] = None,
              batch_size: int = 1000,
              cleanup_before_import: bool = True):
    """
    将CSV文件数据导入到指定的Elasticsearch索引（支持导入前清理数据）

    参数:
        cleanup_before_import: 导入前是否清理目标索引数据（True/False）
    """
    # 1. 读取CSV文件
    try:
        df = pd.read_csv(
            csv_file,
            encoding='utf-8',
            na_values=['', 'null'],
            keep_default_na=False
        )
    except Exception as e:
        raise ValueError(f"读取CSV文件失败: {str(e)}")

    # 2. 连接Elasticsearch
    es_url = f"http://{es_host}:{es_port}"
    es_kwargs = {
        "verify_certs": False,
        "ssl_show_warn": False
    }
    if username and password:
        es_kwargs["basic_auth"] = (username, password)

    es = Elasticsearch(es_url, **es_kwargs)

    if not es.ping():
        raise ConnectionError("无法连接到目标Elasticsearch服务")

    # 3. 导入前清理数据
    if cleanup_before_import:
        if es.indices.exists(index=target_index):
            print(f"开始清理索引 '{target_index}' 中的现有数据...")

            # # 方式1：删除索引并重建（适用于需要重置映射的场景）
            # es.indices.delete(index=target_index)
            # print(f"已删除索引 '{target_index}'")
            #
            # # 重建索引（使用指定映射或默认映射）
            # if mapping:
            #     es.indices.create(index=target_index, body=mapping)
            #     print(f"使用指定映射重建索引 '{target_index}'")
            # else:
            #     es.indices.create(index=target_index)
            #     print(f"使用默认映射重建索引 '{target_index}'")

            # 方式2：仅清空数据（保留索引结构，取消上面注释并注释方式1即可）
            es.delete_by_query(index=target_index, body={"query": {"match_all": {}}})
            print(f"已清空索引 '{target_index}' 中的数据")
        else:
            print(f"索引 '{target_index}' 不存在，无需清理")

    # 4. 若索引仍不存在则创建（适用于首次导入或未清理的场景）
    if not es.indices.exists(index=target_index):
        if mapping:
            es.indices.create(index=target_index, body=mapping)
            print(f"使用指定映射创建新索引 '{target_index}'")
        else:
            es.indices.create(index=target_index)
            print(f"使用默认映射创建新索引 '{target_index}'")

    # 5. 批量导入数据
    total_docs = len(df)
    success_count = 0

    print(f"开始导入数据，共 {total_docs} 条记录...")

    for i in range(0, total_docs, batch_size):
        batch_df = df.iloc[i:i + batch_size]
        actions = []

        for _, row in batch_df.iterrows():
            doc = row.to_dict()
            # 处理空值
            for key, value in doc.items():
                if pd.isna(value):
                    doc[key] = None

            actions.append({
                "_index": target_index,
                "_source": doc
            })

        # 执行批量导入
        success, failed = bulk(es, actions)
        success_count += success

        # 打印进度
        processed = min(i + batch_size, total_docs)
        print(f"已处理 {processed}/{total_docs} 条，成功导入 {success_count} 条")

    print(f"导入完成！总记录数: {total_docs}, 成功导入: {success_count}")


if __name__ == "__main__":
    # 配置参数
    CSV_FILE = "/es_data.csv"  # 要导入的CSV文件路径
    TARGET_INDEX = "all_test"  # 目标索引名称

    ES_HOST = "192.168.199.38"  # Elasticsearch主机
    ES_PORT = 9200  # Elasticsearch端口
    USERNAME = None  # 如有认证，填写用户名
    PASSWORD = None  # 如有认证，填写密码
    # 如需指定导出字段，取消下面一行注释并修改字段列表
    # FIELDS = ["field1", "field2", "field3"]
    FIELDS = None  # 导出所有字段

    # 可选：定义索引映射（推荐，确保字段类型正确）
    # 示例映射（根据CSV实际字段调整）
    MAPPING = {
        "mappings": {
            "properties": {
                # 示例字段：根据你的CSV字段修改
                "id": {"type": "integer"},
                "name": {"type": "text", "fields": {"keyword": {"type": "keyword"}}},
                "age": {"type": "integer"},
                "join_date": {"type": "date", "format": "yyyy-MM-dd"},
                "salary": {"type": "float"}
            }
        }
    }

    # 执行导入
    try:
        csv_to_es(
            csv_file=CSV_FILE,
            es_host=ES_HOST,
            es_port=ES_PORT,
            target_index=TARGET_INDEX,
            username=USERNAME,
            password=PASSWORD,
            mapping=None,  # 如需使用默认映射，可设为None
            batch_size=1000
        )
    except Exception as e:
        print(f"导入失败: {str(e)}")

```
